# -*- coding: utf-8 -*-
"""Week_8_HW.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1cgYNIPhXpVqy3gWqIU-oCmYrX8qjnktG

# 0.) Import and Clean data
"""

import pandas as pd
from google.colab import drive
import matplotlib.pyplot as plt
import numpy as np
import warnings
warnings.filterwarnings("ignore")

from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import BaggingClassifier
from sklearn.datasets import make_classification
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.tree import plot_tree
from sklearn.metrics import confusion_matrix
import seaborn as sns

drive.mount('/content/gdrive/', force_remount = True)

df = pd.read_csv("/content/gdrive/MyDrive/Econ441B/week 8/bank-additional-full.csv", sep = ";")

df.head()

df = df.drop(["default", "pdays",	"previous",	"poutcome",	"emp.var.rate",	"cons.price.idx",	"cons.conf.idx",	"euribor3m",	"nr.employed"], axis = 1)
df = pd.get_dummies(df, columns = ["loan", "job","marital","housing","contact","day_of_week", "campaign", "month", "education"],drop_first = True)

df.head()

y = pd.get_dummies(df["y"], drop_first = True)
X = df.drop(["y"], axis = 1)

obs = len(y)
plt.bar(["No","Yes"],[len(y[y.yes==0])/obs,len(y[y.yes==1])/obs])
plt.ylabel("Percentage of Data")
plt.show()

# Train Test Split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

scaler = StandardScaler().fit(X_train)

X_scaled = scaler.transform(X_train)
X_test = scaler.transform(X_test)

"""#1.) Based on the visualization above, use your expert opinion to transform the data based on what we learned this quarter"""

###############
###TRANSFORM###
###############
from imblearn.over_sampling import RandomOverSampler

ros = RandomOverSampler(random_state=42)
X_scaled, y_train = ros.fit_resample(X_scaled, y_train)
X_test, y_test = ros.fit_resample(X_test, y_test)

"""# 2.) Build and visualize a decision tree of Max Depth 3. Show the confusion matrix."""

dtree = DecisionTreeClassifier(max_depth = 3)
dtree.fit(X_scaled, y_train)

fig, axes = plt.subplots(nrows = 1,ncols = 1,figsize = (4,4), dpi=300)
plot_tree(dtree, filled = True, feature_names = X.columns, class_names=["No","Yes"])

               
#fig.savefig('imagename.png')

"""# 1b.) Confusion matrix on out of sample data. Visualize and store as variable"""

y_pred = dtree.predict(X_test)
y_true = y_test
cm_raw = confusion_matrix(y_true, y_pred)

class_labels = ['Negative', 'Positive']

# Plot the confusion matrix as a heatmap
sns.heatmap(cm_raw, annot=True, fmt='d', cmap='Blues', xticklabels=class_labels, yticklabels=class_labels)
plt.title('Confusion Matrix')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.show()

"""# 3.) Use bagging on your descision tree """

dtree = DecisionTreeClassifier(max_depth = 3)

bagging = BaggingClassifier(estimator=dtree, 
                n_estimators=100, 
                max_samples=0.5, 
                max_features=1.)

bagging.fit(X_scaled, y_train)

y_pred = bagging.predict(X_test)

accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)

y_pred = bagging.predict(X_test)
y_true = y_test
cm_bag = confusion_matrix(y_true, y_pred)

class_labels = ['Negative', 'Positive']

# Plot the confusion matrix as a heatmap
sns.heatmap(cm_bag, annot=True, fmt='d', cmap='Blues', xticklabels=class_labels, yticklabels=class_labels)
plt.title('Confusion Matrix')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.show()

"""# 4.) Boost your tree"""

from sklearn.ensemble import AdaBoostClassifier

dtree = DecisionTreeClassifier(max_depth=3)
adaboost = AdaBoostClassifier(base_estimator=dtree, n_estimators=50, learning_rate=0.1)

adaboost.fit(X_scaled, y_train)

y_pred = adaboost.predict(X_test)

y_pred = adaboost.predict(X_test)
y_true = y_test
cm_boost = confusion_matrix(y_true, y_pred)

class_labels = ['Negative', 'Positive']

# Plot the confusion matrix as a heatmap
sns.heatmap(cm_boost, annot=True, fmt='d', cmap='Blues', xticklabels=class_labels, yticklabels=class_labels)
plt.title('Confusion Matrix')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.show()

"""# 5.) Create a superlearner with at least 5 base learner models. Use a logistic reg for your metalearner. Interpret your coefficients and save your CM."""

!pip install mlens

from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
####IMPORT MORE BASE LEARNERS####

from mlens.ensemble import SuperLearner

### SET YOUR BASE LEARNERS
base_estimators = [
    LogisticRegression(),
    RandomForestClassifier(),
    AdaBoostClassifier(),
    KNeighborsClassifier(5)
]

super_learner = SuperLearner(folds = 10, random_state=42)
super_learner.add(base_estimators)

### FIT TO TRAINING DATA
super_learner.fit(X_scaled, y_train)

### GET base_predictions
base_predictions = super_learner.predict(X_scaled)

### TRAIN YOUR METALEARNER
log_reg = LogisticRegression(fit_intercept=False).fit(base_predictions, y_train)

### INTERPRET COEFFICIENTS
log_reg.coef_

### MAKE, SAVE AND VISUALIZE YOUR CONFUSION MATRIX
y_pred = log_reg.predict(super_learner.predict(X_test))

cm_superlearner = confusion_matrix(y_test, y_pred)

class_labels = ['Negative', 'Positive']

# Plot the confusion matrix as a heatmap
sns.heatmap(cm_superlearner, annot=True, fmt='d', cmap='Blues', xticklabels=class_labels, yticklabels=class_labels)
plt.title('Confusion Matrix')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.show()

"""# 6.) Create a bar chart comparing decision tree, bagged, boosted and super learner Sensitivities (Out of Sample)"""

sen = []
cm_list = [cm_raw, cm_bag, cm_boost, cm_superlearner]
for cm in cm_list:
  tn, fp, fn, tp = cm.ravel()
  sen.append(tp/(tp+fn))

models = ['Decision Tree', 'Bagged', 'Boosted', 'Super Learner']
sensitivity = sen

plt.bar(models,sensitivity,color=['blue','orange','green','red'])
plt.xlabel("Models")
plt.ylabel("Sensitivity (Recall)")
plt.title("Comparison of Sensitivity Values")
plt.show()
