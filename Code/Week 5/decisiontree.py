# -*- coding: utf-8 -*-
"""DecisionTree.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1RuTD_YAnx8bHw6cNAPvecZTSOsAMNd00

# 0.) Import the US Perminent Visas using zip extractor
"""

import pandas as pd
from google.colab import drive
import matplotlib.pyplot as plt
import numpy as np

drive.mount('/content/gdrive/', force_remount = True)

import zipfile

zf = zipfile.ZipFile("/content/gdrive/MyDrive/Econ441B/us_perm_visas.csv.zip") 
df = pd.read_csv(zf.open('us_perm_visas.csv'))

"""# 1.) US perm Visas csv from cycle using zip extractor

"""

df.head()

"""# 2.) Choose 4 features you think are important. Case_status is your target variable

"""

# display column names with a for loop
for col in df.columns:
    print(col)

df["decision_date"] = pd.to_datetime(df.decision_date)
df["d_month"] = df.decision_date.dt.month
df["d_date"] = df.decision_date.dt.day

dta = df[["case_status", "class_of_admission", "wage_offer_from_9089", "d_month", "d_date"]].copy()

"""# 3.) Clean your data for a decision tree

"""

dta.dropna(inplace = True)

dta.class_of_admission.unique()

dta.wage_offer_from_9089.unique()

from sklearn.preprocessing import LabelEncoder

# create a label encoder
le = LabelEncoder()

# fit and transform the string column
dta['class_of_admission'] = le.fit_transform(dta['class_of_admission'])

dta['wage_offer_from_9089'] = dta['wage_offer_from_9089'].astype(str)
dta['wage_offer_from_9089'] = pd.to_numeric(dta['wage_offer_from_9089'].str.replace(',', ''), errors='coerce')
dta.dropna(subset=['wage_offer_from_9089'], inplace=True)
dta['wage_offer_from_9089'] = dta['wage_offer_from_9089'].astype(float)

dta.head()

"""# 4.) Fit and plot a decision tree of depth 3

"""

import pandas as pd
import numpy as np
from sklearn import tree
import matplotlib.pyplot as plt

# split the dataset into features and target
X = dta.drop('case_status', axis=1)
y = dta['case_status']

# fit a decision tree classifier of depth 3
clf = tree.DecisionTreeClassifier(max_depth=3)
clf = clf.fit(X, y)

# Plot the decision tree
plt.figure(figsize=(20, 10))
tree.plot_tree(clf, filled=True, feature_names=X.columns)
plt.show()

X.iloc[:, 2]

"""# 5.) Write your interpretation of the largest (by sample size) leaf node

The largest leaf node is that decision date in $d_{month} \in \{May,\,June,\,Juily,\,August,\,September\}$. ($4.5<d_{month}<=9.5$)

# 6.) Using a for loop, make your own train-test split and determine the best max_depth for out-of sample accuracy
"""

import numpy as np
import pandas as pd
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# split the dataset into features and target
X = dta.drop('case_status', axis=1)
y = dta['case_status']

# Use a for loop to iterate over a range of possible max_depth values
max_depths = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
out_of_sample_accuracies = []
for max_depth in max_depths:
    # Use train_test_split to split the data into training and testing sets
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
    
    # Fit a decision tree model with the current max_depth value
    clf = DecisionTreeClassifier(max_depth=max_depth)
    clf.fit(X_train, y_train)
    
    # Use the model to predict the target values for the test set
    y_pred = clf.predict(X_test)
    
    # Calculate the accuracy of the model on the test set
    accuracy = accuracy_score(y_test, y_pred)
    out_of_sample_accuracies.append(accuracy)

# Find the max_depth that results in the highest out-of-sample accuracy
best_max_depth = max_depths[np.argmax(out_of_sample_accuracies)]
print(best_max_depth)

print(out_of_sample_accuracies)

max(out_of_sample_accuracies)

"""So the best depth is 10, and the best out of sample accuracies is around 80.22%."""
